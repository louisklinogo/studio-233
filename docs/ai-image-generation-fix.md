# AI Image Generation Fix (Gemini & Mastra)

## Issue Overview
Users reported that generating images via the AI Chat Agent failed with the error: `Error: Gemini did not return an image`.
However, generating images via the Canvas UI (TRPC) appeared to work (or failed silently). The agent claimed to have generated an image, but nothing appeared on the canvas.

## Root Causes Identified

### 1. Incorrect Model Name (Backend Failure)
The codebase was using `gemini-3-pro-preview` for image generation in two places:
- `packages/ai/src/workflows/text-to-image.ts` (Agent Tool)
- `apps/web/src/server/trpc/routers/gemini.ts` (Canvas UI)

**The Problem:** `gemini-3-pro-preview` is primarily a text model in the current API. It does not return image files in the response payload when using the standard `generateText` call, causing the "Gemini did not return an image" error.

**The Solution:**
The correct model ID for image generation via Gemini (Imagen 3 pipeline) is **`gemini-3-pro-image-preview`** (or `gemini-2.5-flash-image-preview`).

### 2. Frontend Stream Handling (UI Failure)
Even after fixing the model name, the image generated by the Agent did not appear on the canvas.

**The Problem:**
The frontend component `ChatPanel.tsx` was listening for standard Vercel AI SDK `tool-invocation` parts or a custom `data-canvas-command` type.
However, the Mastra agent stream returns tool executions in a custom format:
- **Type:** `tool-canvasTextToImageTool` (instead of `tool-invocation`)
- **State:** `output-available` (instead of `result`)
- **Structure:** The output is nested: `output: { command: { url: "...", ... } }`

Because `ChatPanel.tsx` didn't recognize this format, it ignored the successful image generation result.

## Fixes Applied

### 1. Updated Model IDs
We updated both the Agent Workflow and the TRPC Router to use the correct image-capable model.

**File:** `packages/ai/src/workflows/text-to-image.ts`
```typescript
// Old
model: google("gemini-3-pro-preview")

// New
model: google("gemini-3-pro-image-preview")
```

**File:** `apps/web/src/server/trpc/routers/gemini.ts`
```typescript
// Old
model: google("gemini-3-pro-preview")

// New
model: google("gemini-3-pro-image-preview")
```

### 2. Updated Chat Parsing Logic
We patched `ChatPanel.tsx` to correctly parse the Mastra tool result format and dispatch the command to the canvas.

**File:** `apps/web/src/components/studio/chat/ChatPanel.tsx`
```typescript
// Added check for Mastra-specific tool type
if (
    (part as any).type === "tool-canvasTextToImageTool" || 
    ((part as any).type === "tool-invocation" && (part as any).toolInvocation?.toolName === "canvasTextToImageTool")
) {
    // ...
    if (state === "output-available" || state === "result") {
        // Handle nested output structure
        const commandData = output?.command || output;
        if (commandData && commandData.url) {
            // Dispatch to canvas
            onCanvasCommand({
                type: "add-image",
                url: commandData.url,
                // ...
            });
        }
    }
}
```

### 3. Tool Output Structure (Agent Tool Bug)

After the stream handling patch landed we still saw the chat claiming success without painting anything. The canvas tool itself (`packages/ai/src/tools/canvas.ts`) was only returning the raw workflow payload (`{ url, width, height }`) instead of the standardized `{ command: { ... } }` envelope, so `ChatPanel` never saw a `command` field to dispatch.

**Fix:** The tool now reuses `text-to-image` workflow's `command` object, merges any prompt/model metadata, streams the same payload via `data-canvas-command`, and returns `{ command }` to conform to `canvasToolOutputSchema`.

### 4. Standardized Command Protocol (Architecture Upgrade)

To prevent future fragility and support new tools without frontend changes, we implemented a standardized protocol for Tool-to-UI communication.

### The Protocol
All UI-facing tools must return a standard JSON structure defined in `packages/ai/src/schemas/tool-output.ts`.

```typescript
import { canvasToolOutputSchema } from "../schemas/tool-output";

// Tool Output Structure
{
  command: {
    type: "add-image" | "update-image" | "add-video",
    url: string,
    // ... command specific data
  },
  // Optional extra data
  data: { ... }
}
```

### Implementation Details
1.  **Shared Schema:** Created `packages/ai/src/schemas/tool-output.ts` using Zod.
2.  **Backend Update:** Refactored `packages/ai/src/workflows/text-to-image.ts` to import this schema and wrap its output in the `command` object.
3.  **Frontend Update:** Refactored `ChatPanel.tsx` to remove tool-specific checks (e.g., `if toolName === 'canvasTextToImageTool'`). It now generically checks for `output.command` in any tool result and dispatches it to the canvas.

This means adding a new tool (e.g., "Generate 3D Model") only requires the backend to return a valid `command`. No frontend changes are needed.

### 5. Hosted Image URLs (Quota / Performance Fix)

Even after the protocol fix, Gemini image generations still returned multi-megabyte `data:` URLs that we echoed back into subsequent agent turns. Each command stuffed the entire base64 payload into the orchestrator stream, rapidly consuming the Gemini-3 Pro token quota and triggering 429 errors.

**Fix:** All workflows, tools, and TRPC helpers now stream the raw bytes directly to Vercel Blob (via `uploadImageBufferToBlob`) and return a short HTTPS URL instead of a data URI. The canvas still receives the full-resolution asset, but the LLM context only sees the lightweight URL, eliminating runaway token usage.

## Verification
1.  **Backend:** The error `Gemini did not return an image` is gone.
2.  **Frontend:** The chat now logs "I generated an image..." AND the image appears on the canvas.
3.  **Model:** We are correctly using `gemini-3-pro-image-preview`.
4.  **Protocol:** The system is using the generic `command` parser.

## Future Recommendations / Next Steps
*   **Migrate Other Tools:** Update `text-to-video` and other workflows to use `canvasToolOutputSchema` and return `command` objects.
*   **Strict Typing:** Enforce the `CanvasCommand` type sharing more strictly between the `ai` package and the `web` app (currently they have duplicate type definitions in `types/canvas.ts`).
*   **Model Config:** Centralize the Gemini model ID string in `packages/ai/src/model-config.ts` so it only needs to be changed in one place.
